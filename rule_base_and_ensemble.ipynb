{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Rerankが全く上手く行かなったので、ルールベースで頑張ることにしました。  \n",
    "候補の選び方は以下の順です。\n",
    "1. 閲覧履歴にある候補を出現順に並べる\n",
    "2. 閲覧履歴の宿と一次の共起関係にある宿を共起数順に並べる\n",
    "3. 閲覧履歴の宿と同一`sml_cd`かつ二次の共起関係にある宿を共起数順に並べる\n",
    "4. 閲覧履歴の宿と異なる`sml_cd`かつ二次の共起関係にある宿を共起数順に並べる\n",
    "5. `sml_cd`・`lrg_cd`・`ken_cd`・`wid_cd`の順に閲覧数の多い宿を並べる\n",
    "\n",
    "\n",
    "2個以上閲覧履歴があるセッションに対してのみRerankすると微増したので、一応最終サブではアンサンブルしています。  \n",
    "(Rerankはマジで大したことしてないので割愛しました。)\n",
    "\n",
    "|              | single history | multi history | total  | LB     |\n",
    "|--------------|----------------|---------------|--------|--------|\n",
    "| CV(Rule)     | 0.14002        | 0.8845        | 0.4064 | 0.4433 |\n",
    "| CV(LGBM)     | Use Rule       | 0.8877        |        | 0.4436 |\n",
    "| CV(Ensemble) |                |               |        | 0.4437 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463398/463398 [00:14<00:00, 31761.60it/s]\n",
      "100%|██████████| 288698/288698 [00:08<00:00, 32602.09it/s]\n",
      "100%|██████████| 174700/174700 [00:05<00:00, 33708.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "AREA_COLS = ['wid_cd', 'ken_cd', 'lrg_cd', 'sml_cd']\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    return list(dict.fromkeys(lst))\n",
    "\n",
    "# suppress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# read csvs\n",
    "train_log = pd.read_csv('../input/train_log.csv')\n",
    "test_log = pd.read_csv('../input/test_log.csv')\n",
    "log = pd.concat([train_log, test_log], axis=0)\n",
    "\n",
    "# convert session_id to session_index\n",
    "unique_ids = log['session_id'].unique()\n",
    "session_index_map = {id:i for i, id in enumerate(unique_ids)}\n",
    "\n",
    "log['session_index'] = log['session_id'].map(session_index_map)\n",
    "train_log['session_index'] = train_log['session_id'].map(session_index_map)\n",
    "test_log['session_index'] = test_log['session_id'].map(session_index_map)\n",
    "\n",
    "# drop session_id\n",
    "log = log.drop('session_id', axis=1)\n",
    "train_log = train_log.drop('session_id', axis=1)\n",
    "test_log = test_log.drop('session_id', axis=1)\n",
    "\n",
    "train_df = train_log[['session_index']].drop_duplicates().reset_index(drop=True)\n",
    "test_df = test_log[['session_index']].drop_duplicates().reset_index(drop=True)\n",
    "df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def factorize_area_cols(yado_df):\n",
    "    # factorize\n",
    "    for col in AREA_COLS:\n",
    "        yado_df[col] = pd.factorize(yado_df[col])[0]\n",
    "    \n",
    "    return yado_df\n",
    "\n",
    "def add_log_features(yado_df, log_df):\n",
    "    # 閲覧回数\n",
    "    yado_df['yad_counts'] = yado_df['yad_no'].map(log_df['yad_no'].value_counts())\n",
    "\n",
    "    # セッション数\n",
    "    yado_df['session_counts'] = yado_df['yad_no'].map(log_df.groupby('yad_no')['session_index'].nunique())\n",
    "\n",
    "    return yado_df\n",
    "\n",
    "yado_df = pd.read_csv('../input/yado.csv')\n",
    "yado_df = yado_df.fillna(0)\n",
    "yado_df = factorize_area_cols(yado_df)\n",
    "\n",
    "yado_df = add_log_features(yado_df.copy(), log)\n",
    "train_yado_df = add_log_features(yado_df.copy(), train_log)\n",
    "test_yado_df = add_log_features(yado_df.copy(), test_log)\n",
    "\n",
    "sml_cd_map = yado_df.set_index('yad_no')['sml_cd'].to_dict()\n",
    "\n",
    "def add_history_features(df, log_df):\n",
    "    log_df.sort_values(['session_index', 'seq_no'], ascending=[True, True], inplace=True)\n",
    "\n",
    "    history_map = defaultdict(list)\n",
    "    for session_index, group in tqdm(log_df.groupby('session_index')):\n",
    "        history_map[session_index] = group['yad_no'].values.tolist()  # group is sorted by seq_no desc\n",
    "\n",
    "    df['histories'] = df['session_index'].map(history_map)\n",
    "    df['num_histories'] = df['histories'].map(len)\n",
    "    df['last_history'] = df['histories'].map(lambda x: x[-1])\n",
    "    df['last_history_sml_cd'] = df['last_history'].map(sml_cd_map)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_history_features(df, log)\n",
    "train_df = add_history_features(train_df, train_log)\n",
    "test_df = add_history_features(test_df, test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History candidates\n",
    "最後のアイテムと同一のアイテムや、重複した予測を除外する条件においては、単に出現順に並べるのが一番いい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_item(lst):\n",
    "    # 最後に出現する要素は除外する\n",
    "    remove_item = lst[-1]\n",
    "    return remove_duplicates([item for item in lst if item != remove_item])\n",
    "\n",
    "def add_history_candidates(df):\n",
    "    df['history_candidates'] = df['histories'].map(remove_last_item)\n",
    "    return df\n",
    "\n",
    "df = add_history_candidates(df)\n",
    "train_df = add_history_candidates(train_df)\n",
    "test_df = add_history_candidates(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-visit Candidates\n",
    "閲覧履歴と共起関係にある宿を候補に入れる。  \n",
    "以下の4つを作成。  \n",
    "- 1次のアイテムを共起数順にソートしたもの\n",
    "- 1次のアイテムを共起数順にソートしたもの（同一セッション内で重複したアイテムは削除）\n",
    "- 2次のアイテムを同一エリア→共起数順にソートしたもの\n",
    "- 2次のアイテムを同一エリア→共起数順にソートしたもの（同一セッション内で重複したアイテムは削除）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164072/164072 [00:17<00:00, 9280.92it/s]\n",
      "100%|██████████| 103312/103312 [00:11<00:00, 9229.11it/s]\n",
      "100%|██████████| 60760/60760 [00:06<00:00, 9166.69it/s]\n",
      "100%|██████████| 164072/164072 [00:54<00:00, 3033.90it/s]\n",
      "100%|██████████| 103312/103312 [00:34<00:00, 2997.33it/s]\n",
      "100%|██████████| 60760/60760 [00:20<00:00, 3012.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_co_visit_graph(log_df, no_duplication=False):\n",
    "    log_df['session_length'] = log_df['session_index'].map(log_df.groupby('session_index').size().to_dict())\n",
    "    log_df2 = log_df.query('session_length > 1')\n",
    "    log_df2 = log_df2.merge(yado_df[['yad_no', ] + AREA_COLS], how='left', on='yad_no')\n",
    "\n",
    "    co_visit_graph = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    area_co_visit_graphs = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for session_index, group in tqdm(log_df2.groupby('session_index')):\n",
    "        if no_duplication:\n",
    "            group = group.drop_duplicates('yad_no')\n",
    "        yados = group['yad_no'].values\n",
    "        \n",
    "        # 宿の共起関係\n",
    "        for yado_id1 in yados:\n",
    "            for yado_id2 in yados:\n",
    "                if yado_id1 == yado_id2:\n",
    "                    continue\n",
    "                co_visit_graph[yado_id1][yado_id2] += 1\n",
    "                co_visit_graph[yado_id2][yado_id1] += 1\n",
    "        \n",
    "        # エリアの共起関係も作っておく\n",
    "        for area_col in AREA_COLS:\n",
    "            areas = group[area_col].values\n",
    "            for area1 in areas:\n",
    "                for area2 in areas:\n",
    "                    if area1 == area2:\n",
    "                        continue\n",
    "                    area_co_visit_graphs[area_col][area1][area2] += 1\n",
    "                    area_co_visit_graphs[area_col][area2][area1] += 1\n",
    "\n",
    "    # sort by count\n",
    "    for history_index in co_visit_graph.keys():\n",
    "        co_visit_graph[history_index] = sorted(co_visit_graph[history_index].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for area_col in area_co_visit_graphs.keys():\n",
    "        for history_index in area_co_visit_graphs[area_col].keys():\n",
    "            area_co_visit_graphs[area_col][history_index] = sorted(area_co_visit_graphs[area_col][history_index].items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return co_visit_graph, area_co_visit_graphs\n",
    "\n",
    "co_visit_graph, area_co_visit_graphs = create_co_visit_graph(log)\n",
    "train_co_visit_graph, train_area_co_visit_graphs = create_co_visit_graph(train_log)\n",
    "test_co_visit_graph, test_area_co_visit_graphs = create_co_visit_graph(test_log)\n",
    "\n",
    "# no duplicated version\n",
    "co_visit_graph_no_dup, area_co_visit_graphs_no_dup = create_co_visit_graph(log, no_duplication=True)\n",
    "train_co_visit_graph_no_dup, train_area_co_visit_graphs_no_dup = create_co_visit_graph(train_log, no_duplication=True)\n",
    "test_co_visit_graph_no_dup, test_area_co_visit_graphs_no_dup = create_co_visit_graph(test_log, no_duplication=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/463398 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463398/463398 [00:05<00:00, 91297.31it/s] \n",
      "100%|██████████| 463398/463398 [00:04<00:00, 110865.13it/s]\n",
      "100%|██████████| 463398/463398 [01:14<00:00, 6204.61it/s]\n",
      "100%|██████████| 463398/463398 [00:11<00:00, 38942.08it/s]\n",
      "100%|██████████| 463398/463398 [01:08<00:00, 6809.00it/s]\n",
      "100%|██████████| 463398/463398 [00:11<00:00, 40764.73it/s]\n",
      "100%|██████████| 288698/288698 [00:02<00:00, 112271.30it/s]\n",
      "100%|██████████| 288698/288698 [00:02<00:00, 115178.68it/s]\n",
      "100%|██████████| 288698/288698 [00:39<00:00, 7353.16it/s]\n",
      "100%|██████████| 288698/288698 [00:07<00:00, 40297.34it/s]\n",
      "100%|██████████| 288698/288698 [00:38<00:00, 7539.19it/s]\n",
      "100%|██████████| 288698/288698 [00:05<00:00, 49205.88it/s]\n",
      "100%|██████████| 174700/174700 [00:01<00:00, 140782.64it/s]\n",
      "100%|██████████| 174700/174700 [00:01<00:00, 144857.28it/s]\n",
      "100%|██████████| 174700/174700 [00:12<00:00, 14313.00it/s]\n",
      "100%|██████████| 174700/174700 [00:02<00:00, 67668.53it/s]\n",
      "100%|██████████| 174700/174700 [00:10<00:00, 16204.33it/s]\n",
      "100%|██████████| 174700/174700 [00:02<00:00, 69956.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_co_visit_candidates(yado_id, co_visit_graph):\n",
    "    if len(co_visit_graph[yado_id]) == 0:\n",
    "        return []\n",
    "    return co_visit_graph[yado_id]\n",
    "\n",
    "def add_co_visit_candidates(df, co_visit_graph, col_name, k=10):\n",
    "    co_visit_candidates = []\n",
    "    for i, history in enumerate(tqdm(df['histories'].values)):\n",
    "        history = remove_duplicates(history)\n",
    "        candidates = []\n",
    "        for yado_id in history:\n",
    "            candidates_ = get_co_visit_candidates(yado_id, co_visit_graph)\n",
    "            candidates_ = [x for x in candidates_ if x[0] not in history][:k]\n",
    "            candidates.extend(candidates_)\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True) # sort by count\n",
    "        candidates = [x[0] for x in candidates] # drop count\n",
    "        candidates = remove_duplicates(candidates)\n",
    "        co_visit_candidates.append(candidates) \n",
    "\n",
    "    df[col_name] = co_visit_candidates\n",
    "    return df\n",
    "\n",
    "def add_second_co_visit_candidates(df, co_visit_graph, col_name, k=10):\n",
    "    second_co_visit_candidates = []\n",
    "    for i, (history, co_visit_candidates) in enumerate(tqdm(df[['histories', 'co_visit_candidates']].values)):\n",
    "        candidates = []\n",
    "        for yado_id in co_visit_candidates:\n",
    "            candidates_ = get_co_visit_candidates(yado_id, co_visit_graph)\n",
    "            candidates_ = [x for x in candidates_ if (x[0] not in history) and (x[0] not in co_visit_candidates)][:k]\n",
    "            candidates.extend(candidates_)\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True) # sort by count\n",
    "        candidates = [x[0] for x in candidates] # drop count\n",
    "        candidates = remove_duplicates(candidates)\n",
    "        second_co_visit_candidates.append(candidates)\n",
    "    df[col_name] = second_co_visit_candidates\n",
    "\n",
    "    # 2次のco-visitは同一エリアが最初に来るように並べ替える\n",
    "    ordered_second_co_visit_candidates = []\n",
    "    for last_history_sml_cd, co_visit_candidates in tqdm(df[['last_history_sml_cd', col_name]].values):\n",
    "        # rorder co_visit_candidates by prioritize same sml_cd\n",
    "        co_visit_candidates_sml_cds = [sml_cd_map[yado_no] for yado_no in co_visit_candidates]\n",
    "        co_visit_candidates = [x for _, x in sorted(zip(co_visit_candidates_sml_cds, co_visit_candidates), key=lambda x: (x[0]!=last_history_sml_cd, x[0]))]\n",
    "        ordered_second_co_visit_candidates.append(co_visit_candidates)\n",
    "\n",
    "    df[col_name] = ordered_second_co_visit_candidates\n",
    "\n",
    "    return df\n",
    "\n",
    "k = 20\n",
    "df = add_co_visit_candidates(df, co_visit_graph, \"co_visit_candidates\", k=k)\n",
    "df = add_co_visit_candidates(df, co_visit_graph_no_dup, \"co_visit_candidates_no_dup\", k=k)\n",
    "df = add_second_co_visit_candidates(df, co_visit_graph, \"second_co_visit_candidates\", k=k)\n",
    "df = add_second_co_visit_candidates(df, co_visit_graph_no_dup, \"second_co_visit_candidates_no_dup\", k=k)\n",
    "\n",
    "train_df = add_co_visit_candidates(train_df, train_co_visit_graph, \"co_visit_candidates\", k=k)\n",
    "train_df = add_co_visit_candidates(train_df, train_co_visit_graph_no_dup, \"co_visit_candidates_no_dup\", k=k)\n",
    "train_df = add_second_co_visit_candidates(train_df, train_co_visit_graph, \"second_co_visit_candidates\", k=k)\n",
    "train_df = add_second_co_visit_candidates(train_df, train_co_visit_graph_no_dup, \"second_co_visit_candidates_no_dup\", k=k)\n",
    "\n",
    "test_df = add_co_visit_candidates(test_df, test_co_visit_graph, \"co_visit_candidates\", k=k)\n",
    "test_df = add_co_visit_candidates(test_df, test_co_visit_graph_no_dup, \"co_visit_candidates_no_dup\", k=k)\n",
    "test_df = add_second_co_visit_candidates(test_df, test_co_visit_graph, \"second_co_visit_candidates\", k=k)\n",
    "test_df = add_second_co_visit_candidates(test_df, test_co_visit_graph_no_dup, \"second_co_visit_candidates_no_dup\", k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Popular Candidates\n",
    "閲覧履歴と同一エリアの人気宿を追加する。  \n",
    "sml_cdとlrg_cdに対しては共起関係にあるエリアの人気宿も後ろに付け加える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_popular_candidates_by_area(train_df, yado_df, area_col, area_co_visit_graphs, k=10, add_co_visit_area_candidates=False):\n",
    "    \n",
    "    area_map = yado_df[['yad_no', area_col]].set_index('yad_no')[area_col].to_dict()\n",
    "    area_group_map = {}\n",
    "    for area_id, area_df in yado_df.groupby(area_col):\n",
    "        topk_area_df = area_df.sort_values('yad_counts', ascending=False).head(k)\n",
    "        area_group_map[area_id] = topk_area_df['yad_no'].tolist()\n",
    "\n",
    "    candidates = []\n",
    "    for i, history in enumerate(tqdm(train_df['histories'].values)):\n",
    "        history = remove_duplicates(history)\n",
    "        areas = set([area_map[x] for x in history])\n",
    "        candidate = []\n",
    "        for area in areas:\n",
    "            candidate.extend(area_group_map[area])\n",
    "        if add_co_visit_area_candidates:\n",
    "            for area in areas:\n",
    "                if isinstance(area_co_visit_graphs[area_col][area] , list):\n",
    "                    covisit_area_counts = area_co_visit_graphs[area_col][area]\n",
    "                    for covisit_area, count in covisit_area_counts:\n",
    "                        candidate.extend(area_group_map[covisit_area])\n",
    "        candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 463398/463398 [00:00<00:00, 722671.98it/s]\n",
      "100%|██████████| 288698/288698 [00:00<00:00, 674034.71it/s]\n",
      "100%|██████████| 174700/174700 [00:01<00:00, 104210.07it/s]\n",
      "100%|██████████| 463398/463398 [00:00<00:00, 692528.36it/s]\n",
      "100%|██████████| 288698/288698 [00:00<00:00, 668985.50it/s]\n",
      "100%|██████████| 174700/174700 [00:00<00:00, 658667.83it/s]\n",
      "100%|██████████| 463398/463398 [00:02<00:00, 180487.74it/s]\n",
      "100%|██████████| 288698/288698 [00:03<00:00, 83294.80it/s] \n",
      "100%|██████████| 174700/174700 [00:00<00:00, 210425.97it/s]\n",
      "100%|██████████| 463398/463398 [00:02<00:00, 175645.99it/s]\n",
      "100%|██████████| 288698/288698 [00:01<00:00, 210286.23it/s]\n",
      "100%|██████████| 174700/174700 [00:00<00:00, 217289.36it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "for area_col in AREA_COLS:\n",
    "    add_co_visit_area_candidates = area_col in ['sml_cd', 'lrg_cd']\n",
    "    df[f'{area_col}_top{k}_candidates'] = get_topk_popular_candidates_by_area(df, yado_df, area_col, area_co_visit_graphs, k, add_co_visit_area_candidates)\n",
    "    train_df[f'{area_col}_top{k}_candidates'] = get_topk_popular_candidates_by_area(train_df, train_yado_df, area_col, train_area_co_visit_graphs, k, add_co_visit_area_candidates)\n",
    "    test_df[f'{area_col}_top{k}_candidates'] = get_topk_popular_candidates_by_area(test_df, test_yado_df, area_col, test_area_co_visit_graphs, k, add_co_visit_area_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess\n",
    "セッション内の最後の宿を予測から削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'history_candidates', \n",
    "    \"co_visit_candidates\",\n",
    "    \"co_visit_candidates_no_dup\",\n",
    "    \"second_co_visit_candidates\",\n",
    "    \"second_co_visit_candidates_no_dup\",\n",
    "    f'sml_cd_top{k}_candidates', \n",
    "    f'lrg_cd_top{k}_candidates', \n",
    "    f'ken_cd_top{k}_candidates', \n",
    "    f'wid_cd_top{k}_candidates'\n",
    "]\n",
    "\n",
    "def remove_last_history_from_candidates(row):\n",
    "    remove_id  = row['histories'][-1]\n",
    "    for col in cols:\n",
    "        row[col] = [x for x in row[col] if x != remove_id]\n",
    "    return row\n",
    "\n",
    "df = df.apply(remove_last_history_from_candidates, axis=1)\n",
    "train_df = train_df.apply(remove_last_history_from_candidates, axis=1)\n",
    "test_df = test_df.apply(remove_last_history_from_candidates, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "co-visit candidatesは以下の4つをアンサンブルして並べ替える\n",
    "- 重複を許すもの(train/testそれぞれ作成)\n",
    "- 重複を許すもの(train/testをconcatして作成)\n",
    "- 重複を許さないもの(train/testそれぞれ作成)\n",
    "- 重複を許さないもの(train/testをconcatして作成)\n",
    "\n",
    "\n",
    "area candidatesは以下の2つをアンサンブルして並べ替える\n",
    "- train/testそれぞれ作成\n",
    "- train/testをconcatして作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def ensemble(row, candidate_cols, weights):\n",
    "    assert len(candidate_cols) == len(weights)\n",
    "\n",
    "    res = defaultdict(float)\n",
    "    for col, weight in zip(candidate_cols, weights):\n",
    "        for n, item in enumerate(row[col], start=1):\n",
    "            res[item] += weight / n\n",
    "    \n",
    "    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n",
    "\n",
    "    return res       \n",
    "\n",
    "def create_submission(df, each_df, session_df, prefix='test'):\n",
    "    sub_df = session_df.merge(df, on='session_index', how='left')\n",
    "    temp_each_df = session_df.merge(each_df, on='session_index', how='left')\n",
    "\n",
    "    co_visit_cols = [\n",
    "        \"co_visit_candidates\",\n",
    "        \"co_visit_candidates_no_dup\",\n",
    "    ]\n",
    "    second_co_visit_cols = [\n",
    "        \"second_co_visit_candidates\",\n",
    "        \"second_co_visit_candidates_no_dup\",\n",
    "    ]\n",
    "    area_cols = [\n",
    "        f'sml_cd_top{k}_candidates', \n",
    "        f'lrg_cd_top{k}_candidates', \n",
    "        f'ken_cd_top{k}_candidates', \n",
    "        f'wid_cd_top{k}_candidates'\n",
    "    ]\n",
    "    cols = co_visit_cols + second_co_visit_cols + area_cols\n",
    "\n",
    "    for col in cols:\n",
    "        sub_df[f'{prefix}_{col}'] = temp_each_df[col]\n",
    "\n",
    "    co_visit_cols += [f'{prefix}_{x}' for x in co_visit_cols]\n",
    "    second_co_visit_cols += [f'{prefix}_{x}' for x in second_co_visit_cols]\n",
    "\n",
    "    sub_df['ensemble_co_visit_candidates'] = sub_df.progress_apply(lambda x: ensemble(x, co_visit_cols, [1.0] * len(co_visit_cols)), axis=1)\n",
    "    sub_df['ensemble_second_co_visit_candidates'] = sub_df.progress_apply(lambda x: ensemble(x, second_co_visit_cols, [1.0] * len(second_co_visit_cols)), axis=1)\n",
    "    for area_col in area_cols:\n",
    "        sub_df[f'ensemble_{area_col}'] = sub_df.progress_apply(lambda x: ensemble(x, [area_col, f\"{prefix}_{area_col}\"], [1.0] * 2), axis=1)\n",
    "    \n",
    "    candidate_cols = [\n",
    "        'history_candidates', \n",
    "        'ensemble_co_visit_candidates', \n",
    "        'ensemble_second_co_visit_candidates', \n",
    "        f'ensemble_sml_cd_top{k}_candidates', \n",
    "        f'ensemble_lrg_cd_top{k}_candidates', \n",
    "        f'ensemble_ken_cd_top{k}_candidates'\n",
    "    ]\n",
    "\n",
    "    sub_df['candidates'] = [[]] * len(sub_df)\n",
    "    for col in candidate_cols:\n",
    "        if isinstance(sub_df[col].iloc[0], np.ndarray):\n",
    "            sub_df[col] = sub_df[col].map(lambda x: x.tolist())\n",
    "        sub_df['candidates'] += sub_df[col]\n",
    "        \n",
    "    sub_df['candidates'] = sub_df['candidates'].map(remove_duplicates)\n",
    "\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288698/288698 [00:10<00:00, 27449.22it/s]\n",
      "100%|██████████| 288698/288698 [00:22<00:00, 12948.55it/s]\n",
      "100%|██████████| 288698/288698 [00:35<00:00, 8118.35it/s]\n",
      "100%|██████████| 288698/288698 [00:38<00:00, 7584.13it/s]\n",
      "100%|██████████| 288698/288698 [00:13<00:00, 20894.35it/s]\n",
      "100%|██████████| 288698/288698 [00:06<00:00, 41501.63it/s]\n",
      "100%|██████████| 174700/174700 [00:06<00:00, 29042.23it/s]\n",
      "100%|██████████| 174700/174700 [00:19<00:00, 8862.23it/s] \n",
      "100%|██████████| 174700/174700 [00:20<00:00, 8631.21it/s]\n",
      "100%|██████████| 174700/174700 [00:22<00:00, 7803.00it/s]\n",
      "100%|██████████| 174700/174700 [00:04<00:00, 41503.44it/s]\n",
      "100%|██████████| 174700/174700 [00:04<00:00, 41300.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sessions = pd.read_csv('../input/train_label.csv')\n",
    "train_sessions = train_sessions.rename(columns={'yad_no': 'label'})\n",
    "train_sessions['session_index'] = train_sessions['session_id'].map(session_index_map)\n",
    "\n",
    "test_sessions = pd.read_csv('../input/test_session.csv')\n",
    "test_sessions['session_index'] = test_sessions['session_id'].map(session_index_map)\n",
    "\n",
    "train_sub_df = create_submission(df, train_df, train_sessions, prefix='train')\n",
    "test_sub_df = create_submission(df, test_df, test_sessions, prefix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapk single: 0.14002880562032816\n",
      "recall single @10: 0.39840656791775\n",
      "recall single @20: 0.5923586462839696\n",
      "recall single @30: 0.6999881328687172\n",
      "recall single @40: 0.7679220653123753\n",
      "recall single @50: 0.8134648786855534\n",
      "mapk multi: 0.884480390421617\n",
      "recall multi @10: 0.9870779773888803\n",
      "recall multi @20: 0.9979770017035775\n",
      "recall multi @30: 0.9992643642558464\n",
      "recall multi @40: 0.9995741056218058\n",
      "recall multi @50: 0.9996805792163543\n",
      "mapk: 0.4064344687319214\n",
      "recall @10: 0.6090655286839535\n",
      "recall @20: 0.7375111708428878\n",
      "recall @30: 0.8070856050267061\n",
      "recall @40: 0.8508198879105501\n",
      "recall @50: 0.8801030834990198\n",
      "num average candidates: 339.281359760026\n",
      "num total candidates: 97949850\n"
     ]
    }
   ],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single actual value.\n",
    "\n",
    "    Parameters:\n",
    "    actual : int\n",
    "        The actual value that is to be predicted\n",
    "    predicted : list\n",
    "        A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The average precision at k\n",
    "    \"\"\"\n",
    "    if actual in predicted[:k]:\n",
    "        return 1.0 / (predicted[:k].index(actual) + 1)\n",
    "    return 0.0\n",
    "\n",
    "def calc_mapk(df, col='candidates', k=10):\n",
    "    df['score'] = [apk(a, p, k) for a, p in zip(df['label'].values, df[col].values)]\n",
    "    return df['score'].mean()\n",
    "\n",
    "def calc_recall(df, col='candidates', topk=10):\n",
    "    df['in_candidates'] = df.apply(lambda x: x['label'] in x[col][:topk], axis=1)\n",
    "    return df['in_candidates'].sum() / len(df)\n",
    "\n",
    "def eval_candidates(df, eval_for_single_history=True, eval_for_multi_history=True):\n",
    "    df['history_length'] = df['histories'].map(len)\n",
    "\n",
    "    # historyが1つしかないケースのrecall\n",
    "    if eval_for_single_history:\n",
    "        single_df = df.query('history_length == 1')\n",
    "        print(\"mapk single:\", calc_mapk(single_df, col='candidates'))\n",
    "        print(\"recall single @10:\", calc_recall(single_df, col='candidates', topk=10))\n",
    "        print(\"recall single @20:\", calc_recall(single_df, col='candidates', topk=20))\n",
    "        print(\"recall single @30:\", calc_recall(single_df, col='candidates', topk=30))\n",
    "        print(\"recall single @40:\", calc_recall(single_df, col='candidates', topk=40))\n",
    "        print(\"recall single @50:\", calc_recall(single_df, col='candidates', topk=50))\n",
    "\n",
    "    # historyが二つ以上あるケースのrecall\n",
    "    if eval_for_multi_history:\n",
    "        multi_df = df.query('history_length > 1')\n",
    "        print(\"mapk multi:\", calc_mapk(multi_df, col='candidates'))\n",
    "        print(\"recall multi @10:\", calc_recall(multi_df, col='candidates', topk=10))\n",
    "        print(\"recall multi @20:\", calc_recall(multi_df, col='candidates', topk=20))\n",
    "        print(\"recall multi @30:\", calc_recall(multi_df, col='candidates', topk=30))\n",
    "        print(\"recall multi @40:\", calc_recall(multi_df, col='candidates', topk=40))\n",
    "        print(\"recall multi @50:\", calc_recall(multi_df, col='candidates', topk=50))\n",
    "\n",
    "    # トータルのrecall\n",
    "    print(\"mapk:\", calc_mapk(df, col='candidates'))\n",
    "    print(\"recall @10:\", calc_recall(df, col='candidates', topk=10))\n",
    "    print(\"recall @20:\", calc_recall(df, col='candidates', topk=20))\n",
    "    print(\"recall @30:\", calc_recall(df, col='candidates', topk=30))\n",
    "    print(\"recall @40:\", calc_recall(df, col='candidates', topk=40))\n",
    "    print(\"recall @50:\", calc_recall(df, col='candidates', topk=50))\n",
    "\n",
    "    # 候補数\n",
    "    print(\"num average candidates:\", df['candidates'].map(len).mean())\n",
    "    print(\"num total candidates:\", df['candidates'].map(len).sum())\n",
    "\n",
    "eval_candidates(train_sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_0</th>\n",
       "      <th>predict_1</th>\n",
       "      <th>predict_2</th>\n",
       "      <th>predict_3</th>\n",
       "      <th>predict_4</th>\n",
       "      <th>predict_5</th>\n",
       "      <th>predict_6</th>\n",
       "      <th>predict_7</th>\n",
       "      <th>predict_8</th>\n",
       "      <th>predict_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3560</td>\n",
       "      <td>4420</td>\n",
       "      <td>11561</td>\n",
       "      <td>5466</td>\n",
       "      <td>9534</td>\n",
       "      <td>4714</td>\n",
       "      <td>5785</td>\n",
       "      <td>2680</td>\n",
       "      <td>4545</td>\n",
       "      <td>6488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143</td>\n",
       "      <td>4066</td>\n",
       "      <td>613</td>\n",
       "      <td>11923</td>\n",
       "      <td>8108</td>\n",
       "      <td>7014</td>\n",
       "      <td>6129</td>\n",
       "      <td>6555</td>\n",
       "      <td>10095</td>\n",
       "      <td>11237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>757</td>\n",
       "      <td>7710</td>\n",
       "      <td>9190</td>\n",
       "      <td>9910</td>\n",
       "      <td>1774</td>\n",
       "      <td>410</td>\n",
       "      <td>13570</td>\n",
       "      <td>3400</td>\n",
       "      <td>10485</td>\n",
       "      <td>6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12341</td>\n",
       "      <td>6991</td>\n",
       "      <td>13521</td>\n",
       "      <td>3359</td>\n",
       "      <td>1542</td>\n",
       "      <td>2363</td>\n",
       "      <td>10861</td>\n",
       "      <td>4180</td>\n",
       "      <td>2795</td>\n",
       "      <td>5080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2862</td>\n",
       "      <td>9020</td>\n",
       "      <td>763</td>\n",
       "      <td>10826</td>\n",
       "      <td>11480</td>\n",
       "      <td>13235</td>\n",
       "      <td>5372</td>\n",
       "      <td>5650</td>\n",
       "      <td>9623</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13292</td>\n",
       "      <td>3811</td>\n",
       "      <td>11214</td>\n",
       "      <td>10857</td>\n",
       "      <td>7202</td>\n",
       "      <td>12785</td>\n",
       "      <td>5624</td>\n",
       "      <td>6178</td>\n",
       "      <td>3701</td>\n",
       "      <td>5066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11776</td>\n",
       "      <td>8041</td>\n",
       "      <td>8691</td>\n",
       "      <td>1462</td>\n",
       "      <td>850</td>\n",
       "      <td>28</td>\n",
       "      <td>3947</td>\n",
       "      <td>638</td>\n",
       "      <td>1089</td>\n",
       "      <td>2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10904</td>\n",
       "      <td>11201</td>\n",
       "      <td>7537</td>\n",
       "      <td>2824</td>\n",
       "      <td>10606</td>\n",
       "      <td>768</td>\n",
       "      <td>5015</td>\n",
       "      <td>13347</td>\n",
       "      <td>2806</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3244</td>\n",
       "      <td>10541</td>\n",
       "      <td>682</td>\n",
       "      <td>3901</td>\n",
       "      <td>4522</td>\n",
       "      <td>3101</td>\n",
       "      <td>9717</td>\n",
       "      <td>2645</td>\n",
       "      <td>7102</td>\n",
       "      <td>13394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5203</td>\n",
       "      <td>12918</td>\n",
       "      <td>11586</td>\n",
       "      <td>2322</td>\n",
       "      <td>11450</td>\n",
       "      <td>4424</td>\n",
       "      <td>1013</td>\n",
       "      <td>11364</td>\n",
       "      <td>9405</td>\n",
       "      <td>5357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predict_0  predict_1  predict_2  predict_3  predict_4  predict_5  \\\n",
       "0       3560       4420      11561       5466       9534       4714   \n",
       "1        143       4066        613      11923       8108       7014   \n",
       "2        757       7710       9190       9910       1774        410   \n",
       "3      12341       6991      13521       3359       1542       2363   \n",
       "4       2862       9020        763      10826      11480      13235   \n",
       "5      13292       3811      11214      10857       7202      12785   \n",
       "6      11776       8041       8691       1462        850         28   \n",
       "7      10904      11201       7537       2824      10606        768   \n",
       "8       3244      10541        682       3901       4522       3101   \n",
       "9       5203      12918      11586       2322      11450       4424   \n",
       "\n",
       "   predict_6  predict_7  predict_8  predict_9  \n",
       "0       5785       2680       4545       6488  \n",
       "1       6129       6555      10095      11237  \n",
       "2      13570       3400      10485       6721  \n",
       "3      10861       4180       2795       5080  \n",
       "4       5372       5650       9623       1448  \n",
       "5       5624       6178       3701       5066  \n",
       "6       3947        638       1089       2422  \n",
       "7       5015      13347       2806       3483  \n",
       "8       9717       2645       7102      13394  \n",
       "9       1013      11364       9405       5357  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "for i in range(10):\n",
    "    sub[f'predict_{i}'] = test_sub_df['candidates'].map(lambda x: x[i] if len(x) > i else -1)\n",
    "sub.to_csv('output/rule_based.csv', index=False)\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based = test_sub_df[['session_index', 'candidates']].reset_index(drop=True)\n",
    "\n",
    "re_rank = pd.read_pickle('output/test_pred_df_re_rank.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sessioin_index = re_rank['session_index'].unique().tolist()\n",
    "\n",
    "multi_rule_based = rule_based[rule_based['session_index'].isin(multi_sessioin_index)]\n",
    "single_rule_based = rule_based[~rule_based['session_index'].isin(multi_sessioin_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_index</th>\n",
       "      <th>rule_based_candidates</th>\n",
       "      <th>re_rank_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288698</td>\n",
       "      <td>[3560, 4420, 11561, 5466, 9534, 4714, 5785, 26...</td>\n",
       "      <td>[3560, 4714, 4545, 9534, 11561, 4420, 5466, 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288700</td>\n",
       "      <td>[757, 7710, 9190, 9910, 1774, 410, 13570, 3400...</td>\n",
       "      <td>[757, 9190, 7710, 9910, 410, 1774, 10485, 6721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288701</td>\n",
       "      <td>[12341, 6991, 13521, 3359, 1542, 2363, 10861, ...</td>\n",
       "      <td>[12341, 6991, 3359, 13521, 1542, 5080, 4180, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288705</td>\n",
       "      <td>[10904, 11201, 7537, 2824, 10606, 768, 5015, 1...</td>\n",
       "      <td>[10904, 11201, 7537]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>288709</td>\n",
       "      <td>[12986, 12089, 11037, 5944, 6199, 2927, 4614, ...</td>\n",
       "      <td>[12986, 8468, 12089, 2452, 12939, 4614, 10155,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60755</th>\n",
       "      <td>463377</td>\n",
       "      <td>[8827, 13552, 11436, 11476, 9709, 13547, 7228,...</td>\n",
       "      <td>[8827, 11436, 11476, 13552, 12577, 13547, 1184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60756</th>\n",
       "      <td>463381</td>\n",
       "      <td>[3720, 7222, 10412, 5393, 10621, 13736, 12579,...</td>\n",
       "      <td>[3720, 7222, 5393, 10412, 3158, 1342, 7812, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60757</th>\n",
       "      <td>463382</td>\n",
       "      <td>[4834, 10418, 10883, 2028, 7618, 8107, 3739, 8...</td>\n",
       "      <td>[4834, 10418, 10883, 7618, 2028, 7391, 13306, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60758</th>\n",
       "      <td>463390</td>\n",
       "      <td>[11561, 2680, 5785, 1959, 9534, 6563, 6058, 98...</td>\n",
       "      <td>[11561, 2680, 1959, 5785, 9534, 5466, 6563, 47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60759</th>\n",
       "      <td>463393</td>\n",
       "      <td>[1997, 7888, 1885, 11123, 8771, 7641, 831, 574...</td>\n",
       "      <td>[1997, 7888, 11123, 1885, 831, 5744, 10997, 70...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60760 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session_index                              rule_based_candidates  \\\n",
       "0             288698  [3560, 4420, 11561, 5466, 9534, 4714, 5785, 26...   \n",
       "1             288700  [757, 7710, 9190, 9910, 1774, 410, 13570, 3400...   \n",
       "2             288701  [12341, 6991, 13521, 3359, 1542, 2363, 10861, ...   \n",
       "3             288705  [10904, 11201, 7537, 2824, 10606, 768, 5015, 1...   \n",
       "4             288709  [12986, 12089, 11037, 5944, 6199, 2927, 4614, ...   \n",
       "...              ...                                                ...   \n",
       "60755         463377  [8827, 13552, 11436, 11476, 9709, 13547, 7228,...   \n",
       "60756         463381  [3720, 7222, 10412, 5393, 10621, 13736, 12579,...   \n",
       "60757         463382  [4834, 10418, 10883, 2028, 7618, 8107, 3739, 8...   \n",
       "60758         463390  [11561, 2680, 5785, 1959, 9534, 6563, 6058, 98...   \n",
       "60759         463393  [1997, 7888, 1885, 11123, 8771, 7641, 831, 574...   \n",
       "\n",
       "                                      re_rank_candidates  \n",
       "0      [3560, 4714, 4545, 9534, 11561, 4420, 5466, 26...  \n",
       "1      [757, 9190, 7710, 9910, 410, 1774, 10485, 6721...  \n",
       "2      [12341, 6991, 3359, 13521, 1542, 5080, 4180, 1...  \n",
       "3                                   [10904, 11201, 7537]  \n",
       "4      [12986, 8468, 12089, 2452, 12939, 4614, 10155,...  \n",
       "...                                                  ...  \n",
       "60755  [8827, 11436, 11476, 13552, 12577, 13547, 1184...  \n",
       "60756  [3720, 7222, 5393, 10412, 3158, 1342, 7812, 10...  \n",
       "60757  [4834, 10418, 10883, 7618, 2028, 7391, 13306, ...  \n",
       "60758  [11561, 2680, 1959, 5785, 9534, 5466, 6563, 47...  \n",
       "60759  [1997, 7888, 11123, 1885, 831, 5744, 10997, 70...  \n",
       "\n",
       "[60760 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_df = pd.DataFrame({'session_index': multi_sessioin_index})\n",
    "ensemble_df = ensemble_df.merge(multi_rule_based, on='session_index', how='left').rename(columns={'candidates': 'rule_based_candidates'})\n",
    "ensemble_df = ensemble_df.merge(re_rank, on='session_index', how='left').rename(columns={'candidates': 're_rank_candidates'})\n",
    "ensemble_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60760/60760 [00:09<00:00, 6494.09it/s]\n"
     ]
    }
   ],
   "source": [
    "cols = ['rule_based_candidates', 're_rank_candidates']\n",
    "weights = [0.49, 0.51]\n",
    "ensemble_df['candidates'] = ensemble_df.progress_apply(lambda x: ensemble(x, cols, weights), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['session_index', 'candidates']\n",
    "pred_df = pd.concat([single_rule_based[cols], ensemble_df[cols]]).reset_index(drop=True)\n",
    "pred_df.sort_values('session_index', inplace=True)\n",
    "pred_df = pred_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_0</th>\n",
       "      <th>predict_1</th>\n",
       "      <th>predict_2</th>\n",
       "      <th>predict_3</th>\n",
       "      <th>predict_4</th>\n",
       "      <th>predict_5</th>\n",
       "      <th>predict_6</th>\n",
       "      <th>predict_7</th>\n",
       "      <th>predict_8</th>\n",
       "      <th>predict_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3560</td>\n",
       "      <td>4714</td>\n",
       "      <td>4420</td>\n",
       "      <td>11561</td>\n",
       "      <td>9534</td>\n",
       "      <td>4545</td>\n",
       "      <td>5466</td>\n",
       "      <td>5785</td>\n",
       "      <td>2680</td>\n",
       "      <td>6488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143</td>\n",
       "      <td>4066</td>\n",
       "      <td>613</td>\n",
       "      <td>11923</td>\n",
       "      <td>8108</td>\n",
       "      <td>7014</td>\n",
       "      <td>6129</td>\n",
       "      <td>6555</td>\n",
       "      <td>10095</td>\n",
       "      <td>11237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>757</td>\n",
       "      <td>9190</td>\n",
       "      <td>7710</td>\n",
       "      <td>9910</td>\n",
       "      <td>410</td>\n",
       "      <td>1774</td>\n",
       "      <td>10485</td>\n",
       "      <td>13570</td>\n",
       "      <td>6721</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12341</td>\n",
       "      <td>6991</td>\n",
       "      <td>3359</td>\n",
       "      <td>13521</td>\n",
       "      <td>1542</td>\n",
       "      <td>4180</td>\n",
       "      <td>5080</td>\n",
       "      <td>10861</td>\n",
       "      <td>2363</td>\n",
       "      <td>10746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2862</td>\n",
       "      <td>9020</td>\n",
       "      <td>763</td>\n",
       "      <td>10826</td>\n",
       "      <td>11480</td>\n",
       "      <td>13235</td>\n",
       "      <td>5372</td>\n",
       "      <td>5650</td>\n",
       "      <td>9623</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13292</td>\n",
       "      <td>3811</td>\n",
       "      <td>11214</td>\n",
       "      <td>10857</td>\n",
       "      <td>7202</td>\n",
       "      <td>12785</td>\n",
       "      <td>5624</td>\n",
       "      <td>6178</td>\n",
       "      <td>3701</td>\n",
       "      <td>5066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11776</td>\n",
       "      <td>8041</td>\n",
       "      <td>8691</td>\n",
       "      <td>1462</td>\n",
       "      <td>850</td>\n",
       "      <td>28</td>\n",
       "      <td>3947</td>\n",
       "      <td>638</td>\n",
       "      <td>1089</td>\n",
       "      <td>2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10904</td>\n",
       "      <td>11201</td>\n",
       "      <td>7537</td>\n",
       "      <td>2824</td>\n",
       "      <td>10606</td>\n",
       "      <td>768</td>\n",
       "      <td>5015</td>\n",
       "      <td>13347</td>\n",
       "      <td>2806</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3244</td>\n",
       "      <td>10541</td>\n",
       "      <td>682</td>\n",
       "      <td>3901</td>\n",
       "      <td>4522</td>\n",
       "      <td>3101</td>\n",
       "      <td>9717</td>\n",
       "      <td>2645</td>\n",
       "      <td>7102</td>\n",
       "      <td>13394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5203</td>\n",
       "      <td>12918</td>\n",
       "      <td>11586</td>\n",
       "      <td>2322</td>\n",
       "      <td>11450</td>\n",
       "      <td>4424</td>\n",
       "      <td>1013</td>\n",
       "      <td>11364</td>\n",
       "      <td>9405</td>\n",
       "      <td>5357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predict_0  predict_1  predict_2  predict_3  predict_4  predict_5  \\\n",
       "0       3560       4714       4420      11561       9534       4545   \n",
       "1        143       4066        613      11923       8108       7014   \n",
       "2        757       9190       7710       9910        410       1774   \n",
       "3      12341       6991       3359      13521       1542       4180   \n",
       "4       2862       9020        763      10826      11480      13235   \n",
       "5      13292       3811      11214      10857       7202      12785   \n",
       "6      11776       8041       8691       1462        850         28   \n",
       "7      10904      11201       7537       2824      10606        768   \n",
       "8       3244      10541        682       3901       4522       3101   \n",
       "9       5203      12918      11586       2322      11450       4424   \n",
       "\n",
       "   predict_6  predict_7  predict_8  predict_9  \n",
       "0       5466       5785       2680       6488  \n",
       "1       6129       6555      10095      11237  \n",
       "2      10485      13570       6721       3400  \n",
       "3       5080      10861       2363      10746  \n",
       "4       5372       5650       9623       1448  \n",
       "5       5624       6178       3701       5066  \n",
       "6       3947        638       1089       2422  \n",
       "7       5015      13347       2806       3483  \n",
       "8       9717       2645       7102      13394  \n",
       "9       1013      11364       9405       5357  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "for i in range(10):\n",
    "    sub[f'predict_{i}'] = pred_df['candidates'].map(lambda x: x[i] if len(x) > i else -1)\n",
    "\n",
    "sub.to_csv('output/sub_ensemble.csv', index=False)\n",
    "sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
